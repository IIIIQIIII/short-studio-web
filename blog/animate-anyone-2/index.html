<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Animate Anyone 2</title>
  <link rel="icon" type="image/svg+xml" href="favicon.svg">
  
  <!-- TailwindCSS CDN -->
  <script src="https://cdn.tailwindcss.com"></script>
  
  <!-- 自定义Tailwind配置 -->
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            primary: '#3B82F6',
            secondary: '#1E293B',
            accent: '#F59E0B'
          },
          fontFamily: {
            'sans': ['Google Sans', 'Noto Sans', 'sans-serif'],
            'title': ['Castoro', 'serif']
          },
          animation: {
            'fade-in': 'fadeIn 0.5s ease-in-out',
            'slide-up': 'slideUp 0.5s ease-out',
            'pulse-slow': 'pulse 3s infinite'
          },
          keyframes: {
            fadeIn: {
              '0%': { opacity: '0' },
              '100%': { opacity: '1' }
            },
            slideUp: {
              '0%': { transform: 'translateY(20px)', opacity: '0' },
              '100%': { transform: 'translateY(0)', opacity: '1' }
            }
          }
        }
      }
    }
  </script>
  
  <!-- 自定义样式 -->
  <style>
    .video-container {
      position: relative;
      overflow: hidden;
      border-radius: 12px;
      box-shadow: 0 10px 25px rgba(0, 0, 0, 0.1);
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    
    .video-container:hover {
      transform: translateY(-5px);
      box-shadow: 0 15px 30px rgba(0, 0, 0, 0.15);
    }
    
    .carousel-container {
      scroll-behavior: smooth;
      scrollbar-width: none; /* Firefox */
    }
    
    .carousel-container::-webkit-scrollbar {
      display: none; /* Chrome, Safari, Opera */
    }
    
    .gradient-text {
      background: linear-gradient(90deg, #3B82F6, #8B5CF6);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }
    
    .navbar-dropdown {
      opacity: 0;
      visibility: hidden;
      transform: translateY(-10px);
      transition: all 0.3s ease;
    }
    
    .navbar-item:hover .navbar-dropdown {
      opacity: 1;
      visibility: visible;
      transform: translateY(0);
    }

    /* 优化的深色渐变文本效果 */
    .deep-gradient-text {
      background: linear-gradient(135deg, #1a202c 0%, #2d3748 25%, #4a5568 50%, #2d3748 75%, #1a202c 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      background-size: 200% 200%;
      animation: gradient-shift 8s ease infinite;
    }

    /* 新增更多渐变样式选项 */
    .gradient-text-primary {
      background: linear-gradient(135deg, #1e40af 0%, #3730a3 50%, #5b21b6 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }
    
    .gradient-text-elegant {
      background: linear-gradient(135deg, #374151 0%, #4b5563 25%, #6b7280 50%, #4b5563 75%, #374151 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      background-size: 200% 200%;
      animation: gradient-shift 6s ease infinite;
    }
    
    .gradient-text-rich {
      background: linear-gradient(135deg, #111827 0%, #1f2937 33%, #374151 66%, #1f2937 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }

    /* 渐变动画 */
    @keyframes gradient-shift {
      0% { background-position: 0% 50%; }
      50% { background-position: 100% 50%; }
      100% { background-position: 0% 50%; }
    }

    /* 段落文本优化 */
    .gradient-paragraph {
      background: linear-gradient(135deg, #1a202c 0%, #2d3748 30%, #4a5568 50%, #2d3748 70%, #1a202c 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      line-height: 1.7;
    }

    /* 卡片阴影优化以突出渐变文本 */
    .gradient-card {
      background: white;
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.08);
      border: 1px solid rgba(255, 255, 255, 0.2);
    }
  </style>
</head>
<body class="font-sans bg-gray-50 text-gray-800">
  <!-- 导航栏 -->
  <nav class="bg-white shadow-md sticky top-0 z-50">
    <div class="container mx-auto px-4 py-3">
      <div class="flex justify-between items-center">
        <div class="flex items-center">
          <a href="https://short.mashijian.com/" class="flex items-center text-primary hover:text-secondary transition-colors">
            <img src="favicon.svg" alt="HumanAIGC Logo" class="w-6 h-6 mr-2">
            <span class="font-semibold">HumanAIGC</span>
          </a>
        </div>
        
        <div class="flex items-center space-x-6">
          <div class="relative navbar-item hidden md:block">
            <button class="flex items-center text-gray-700 hover:text-primary transition-colors">
              <span>More Research</span>
              <svg class="w-4 h-4 ml-1" fill="currentColor" viewBox="0 0 20 20">
                <path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"/>
              </svg>
            </button>
            
            <div class="navbar-dropdown absolute left-0 mt-2 w-56 bg-white rounded-md shadow-lg py-1 z-50">
              <a href="https://humanaigc.github.io/animate-anyone/" class="block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100">AnimateAnyone</a>
              <a href="https://humanaigc.github.io/OmniTalker/" class="block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100">OmniTalker</a>
              <a href="https://humanaigc.github.io/TalkingBody/" class="block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100">TalkingBody</a>
              <a href="https://humanaigc.github.io/emote-portrait-alive-2/" class="block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100">EMO2</a>
              <a href="https://humanaigc.github.io/emote-portrait-alive/" class="block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100">EMO</a>
              <a href="https://humanaigc.github.io/outfit-anyone/" class="block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100">OutfitAnyone</a>
              <a href="https://tomguluson92.github.io/projects/cloth2tex/" class="block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100">Cloth2Tex</a>
              <a href="https://humanaigc.github.io/vivid-talk/" class="block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100">VividTalk</a>
            </div>
          </div>
          <a href="https://short.mashijian.com/blog" class="text-gray-700 hover:text-primary transition-colors hidden md:block">Blog</a>
        </div>
        
        <!-- 移动端菜单按钮 -->
        <button id="mobile-menu-button" class="md:hidden text-gray-700">
          <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/>
          </svg>
        </button>
      </div>
    </div>
  </nav>

  <!-- 英雄区域 -->
  <section class="py-16 bg-gradient-to-br from-blue-50 to-indigo-100">
    <div class="container mx-auto px-4">
      <div class="max-w-4xl mx-auto text-center">
        <h1 class="text-4xl md:text-6xl font-bold mb-6 animate-fade-in">
          Animate Anyone <span class="gradient-text">2</span>
        </h1>
        <p class="text-xl md:text-2xl text-gray-700 mb-8 animate-slide-up">
          High-Fidelity Character Image Animation with Environment Affordance
        </p>
        
        <div class="flex flex-wrap justify-center gap-4 mb-12">
          <span class="author-block">
            <a href="https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=Arz3iGUAAAAJ&gmla=AJsN-F72u4R_vwVl2Jc0Sy_qIYuSwExx8ilpfrd-w5Yfi5FYFP_WhbJtHbAK_c5w-3KNBgTRjWiTvEFLtJSV5ryd1JuNVQdMVDMuSJS5dfn7NWbZQQpGGyyxlrfoq6cv6S_23QTSUWWY"
              target="_blank" class="text-primary hover:underline">Li Hu</a><sup>*</sup>,
          </span>
          <span class="author-block">
            <a href="https://humanaigc.github.io/animate-anyone-2/" target="_blank" class="text-primary hover:underline">Guangyuan Wang</a><sup>*</sup>,
          </span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=EY9GiDsAAAAJ&hl=en" target="_blank" class="text-primary hover:underline">Zhen Shen</a>,
          </span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=cze1sXQAAAAJ&hl=en" target="_blank" class="text-primary hover:underline">Xin Gao</a>,
          </span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=008R8c0AAAAJ&hl=en" target="_blank" class="text-primary hover:underline">Dechao Meng</a>,
          </span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=2E3c39AAAAAJ&hl=zh-CN" target="_blank" class="text-primary hover:underline">Lian Zhuo</a>,
          </span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=QTgxKmkAAAAJ&hl=zh-CN" target="_blank" class="text-primary hover:underline">Peng Zhang</a>,
          </span>
          <span class="author-block">
            <a href="https://dblp.org/pid/11/4046.html" target="_blank" class="text-primary hover:underline">Bang Zhang</a>,
          </span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=FJwtMf0AAAAJ&hl=zh-CN" target="_blank" class="text-primary hover:underline">Liefeng Bo</a>
          </span>
        </div>
        
        <p class="text-lg text-gray-600 mb-8">Tongyi Lab，Alibaba Group</p>
        
        <div class="flex justify-center">
          <a href="https://arxiv.org/pdf/2502.06145" target="_blank"
            class="inline-flex items-center px-6 py-3 bg-primary text-white font-medium rounded-lg hover:bg-blue-700 transition-colors shadow-md">
            <svg class="w-5 h-5 mr-2" fill="currentColor" viewBox="0 0 24 24">
              <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-1 16h2v-6h-2v6zm2-8V6h-2v4h2z"/>
            </svg>
            <span>arXiv Paper</span>
          </a>
        </div>
      </div>
    </div>
  </section>

  <!-- 预览视频 -->
  <section class="py-16 bg-white">
    <div class="container mx-auto px-4">
      <div class="grid grid-cols-1 md:grid-cols-2 gap-8 mb-8">
        <div class="video-container">
          <video class="w-full h-auto" muted loop controls>
            <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/hoi/handou_res.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-container">
          <video class="w-full h-auto" muted loop controls>
            <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/hoi/huaban_res1.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
        <div class="video-container">
          <video class="w-full h-auto" muted loop controls>
            <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/motion/paoku2_res.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-container">
          <video class="w-full h-auto" muted loop controls>
            <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/motion/wuqiang_res1.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </section>

  <!-- 摘要 -->
  <section class="py-16 bg-gray-100">
    <div class="container mx-auto px-4">
      <div class="max-w-4xl mx-auto">
        <h2 class="text-3xl font-bold text-center mb-8 gradient-text-primary">Abstract</h2>
        <div class="gradient-card p-8 rounded-xl">
          <p class="text-lg leading-relaxed gradient-paragraph">
            Recent character image animation methods based on diffusion models, such as Animate Anyone, have made significant progress in generating consistent and generalizable character animations. However, these approaches fail to produce reasonable associations between characters and their environments. To address this limitation, we introduce Animate Anyone 2, aiming to animate characters with environment affordance. Beyond extracting motion signals from source video, we additionally capture environmental representations as conditional inputs. The environment is formulated as the region with the exclusion of characters and our model generates characters to populate these regions while maintaining coherence with the environmental context. We propose a shape-agnostic mask strategy that more effectively characterizes the relationship between character and environment. Furthermore, to enhance the fidelity of object interactions, we leverage an object guider to extract features of interacting objects and employ spatial blending for feature injection. We also introduce a pose modulation strategy that enables the model to handle more diverse motion patterns. Experimental results demonstrate the superior performance of the proposed method.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- 动机 -->
  <section class="py-16 bg-white">
    <div class="container mx-auto px-4">
      <div class="max-w-4xl mx-auto">
        <h2 class="text-3xl font-bold text-center mb-8 gradient-text-rich">Motivation</h2>
        <div class="flex justify-center mb-8">
          <img src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/images/motivation.png" alt="Motivation" class="rounded-xl shadow-lg max-w-full">
        </div>
        <div class="text-lg leading-relaxed gradient-text-elegant">
          <p>
            We propose Animate Anyone 2, which differs from previous character image animation methods that solely utilize motion signals to animate characters. Our approach additionally extracts environmental representations from the driving video, thereby enabling character animation to exhibit environment affordance.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- 方法 -->
  <section class="py-16 bg-gray-100">
    <div class="container mx-auto px-4">
      <div class="max-w-4xl mx-auto">
        <h2 class="text-3xl font-bold text-center mb-8 gradient-text-primary">Method</h2>
        <div class="flex justify-center mb-8">
          <img src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/images/framework.png" alt="Framework" class="rounded-xl shadow-lg max-w-full">
        </div>
        <div class="text-lg leading-relaxed gradient-paragraph">
          <p>
            The framework of Animate Anyone 2. We capture environmental information from the source video. The environment is formulated as regions devoid of characters and incorporated as model input, enabling end-to-end learning of character-environment fusion. To preserve object interactions, we additionally inject features of objects interacting with the character. These object features are extracted by a lightweight object guider and merged into the denoising process via spatial blending. To handle more diverse motions, we propose a pose modulation approach to better represent the spatial relationships between body limbs.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- 结果展示 -->
  <section class="py-16 bg-white">
    <div class="container mx-auto px-4">
      <h2 class="text-3xl font-bold text-center mb-12 gradient-text-rich">Results</h2>
      
      <!-- 环境交互 -->
      <div class="mb-16">
        <h3 class="text-2xl font-semibold mb-6 gradient-text-elegant">Environment Interaction</h3>
        <div class="carousel-container flex overflow-x-auto space-x-4 pb-4">
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/hoi/handin_res.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/hoi/guitar_res.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/hoi/bicycle_res.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/hoi/horse2_res.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/hoi/chonglang_res.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <p class="mt-4 gradient-text-elegant">
          Animate Anyone 2 demonstrates remarkable capabilities in generating characters with contextually coherent environmental interactions, characterized by seamless character-scene integration and robust character-object interaction.
        </p>
      </div>
      
      <!-- 动态运动 -->
      <div class="mb-16">
        <h3 class="text-2xl font-semibold mb-6 gradient-text-elegant">Dynamic Motion</h3>
        <div class="carousel-container flex overflow-x-auto space-x-4 pb-4">
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/motion/paoku4_res.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/motion/huazi_res.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/hoi/wuqiang_res2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/motion/breaking2_res1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/motion/breaking1_res.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <p class="mt-4 gradient-text-elegant">
          Animate Anyone 2 demonstrates robust capability in handling diverse and intricate motions, while ensuring character consistency and maintaining plausible interactions with the environmental context.
        </p>
      </div>
      
      <!-- 人类交互 -->
      <div class="mb-16">
        <h3 class="text-2xl font-semibold mb-6 gradient-text-elegant">Human Interaction</h3>
        <div class="carousel-container flex overflow-x-auto space-x-4 pb-4">
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/multi/aiyue_res.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/multi/dadou_res.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/multi/titui_res.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/multi/huabin1_res.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/multi/back_res.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <p class="mt-4 gradient-text-elegant">
          Animate Anyone 2 is capable of generating interactions between characters, ensuring the plausibility of their movements and coherence with the surrounding environment.
        </p>
      </div>
    </div>
  </section>

  <!-- 对比 -->
  <section class="py-16 bg-gray-100">
    <div class="container mx-auto px-4">
      <h2 class="text-3xl font-bold text-center mb-12 gradient-text-primary">Comparisons</h2>
      
      <!-- 与Viggle对比 -->
      <div class="mb-16">
        <h3 class="text-2xl font-semibold mb-6 gradient-text-elegant">with Viggle</h3>
        <div class="carousel-container flex overflow-x-auto space-x-4 pb-4">
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/viggle/viggle_com1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/viggle/viggle_com4.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/viggle/viggle_com5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/viggle/viggle_com2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/viggle/viggle_com3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <p class="mt-4 gradient-text-elegant">
          <a href="https://viggle.ai/" class="text-primary hover:underline">Viggle</a> is capable of swapping characters in a video based on a provided character image, which is similar to the application scenario of our method. We compare our results with the latest Viggle V3. The outputs of Viggle demonstrate a rough blending of the characters with the environment, lack natural motion, and fail to capture the interaction between characters and the surroundings. In contrast, the results of our method exhibit higher fidelity.
        </p>
      </div>
      
      <!-- 与MIMO对比 -->
      <div>
        <h3 class="text-2xl font-semibold mb-6 gradient-text-elegant">with MIMO</h3>
        <div class="carousel-container flex overflow-x-auto space-x-4 pb-4">
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/mimo/mimo_com6.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/mimo/mimo_com77.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/mimo/mimo_com3.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-container flex-shrink-0 w-80">
            <video class="w-full h-auto" muted loop controls>
              <source src="https://raw.githubusercontent.com/HumanAIGC/animate-anyone-2/refs/heads/main/static2/videos/mimo/mimo_com55.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <p class="mt-4 gradient-text-elegant">
          <a href="https://menyifang.github.io/projects/MIMO/index.html" class="text-primary hover:underline">MIMO</a> is the most relevant method to our task setting, which decomposes videos into human, background and occlusion based on depth and composing these elements to generate character video. Our approach demonstrates superior robustness and finer detail preservation.
        </p>
      </div>
    </div>
  </section>

  <!-- 页脚 -->
  <footer class="py-8 bg-secondary text-white">
    <div class="container mx-auto px-4">
      <div class="max-w-4xl mx-auto text-center">
        <p class="text-gray-400 text-sm">
          &copy; 2025 Short Studio. All rights reserved.
        </p>
      </div>
    </div>
  </footer>

  <!-- 自定义JavaScript -->
  <script>
    // 移动端菜单切换
    document.getElementById('mobile-menu-button').addEventListener('click', function() {
      const menu = document.querySelector('.navbar-dropdown');
      menu.classList.toggle('hidden');
      menu.classList.toggle('block');
    });
    
    // 视频点击播放效果
    document.querySelectorAll('.video-container').forEach(container => {
      const video = container.querySelector('video');
      
      container.addEventListener('click', () => {
        if (video.paused) {
          video.play();
        } else {
          video.pause();
        }
      });
    });
    
    // 滚动动画
    const observerOptions = {
      threshold: 0.1,
      rootMargin: '0px 0px -50px 0px'
    };
    
    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          entry.target.classList.add('animate-fade-in');
        }
      });
    }, observerOptions);
    
    document.querySelectorAll('section').forEach(section => {
      observer.observe(section);
    });
  </script>
</body>
</html>
